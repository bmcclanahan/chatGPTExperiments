{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d66db-27bc-448b-a372-e93c73ad2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964f7b47-732f-476a-8b2b-1b1f471a9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/state_of_the_union/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2002968d-cf0b-4350-9d98-9e4f8806c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "paths = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
    "texts = [open(path, 'r').read().replace('\\n\\n', '\\n') for path in paths if path.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0888a8-e443-44b3-9563-caaa6bb97b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ee6f38-d941-430c-b58e-1be9a0c06137",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(separator = \".\", chunk_size=3000, chunk_overlap=100)\n",
    "docs = text_splitter.create_documents([texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6007af72-eed5-4a6c-9c62-2b395fb0bc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from typing import Any, Dict, Optional, List\n",
    "from langchain.callbacks.manager import CallbackManagerForChainRun\n",
    "from langchain.chains.summarize import load_summarize_chain, refine_prompts\n",
    "from langchain.schema import LLMResult, Generation\n",
    "\n",
    "\n",
    "class RefineLLMChain(LLMChain):\n",
    "    \"\"\"Chain to run queries against LLMs. Queries can be large and will be broken up and\n",
    "    processed incrementally using the Refine summarize strategy. Only one input key allowed.\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from langchain import LLMChain, OpenAI, PromptTemplate\n",
    "            prompt_template = \"Tell me a {adjective} joke\"\n",
    "            prompt = PromptTemplate(\n",
    "                input_variables=[\"adjective\"], template=prompt_template\n",
    "            )\n",
    "            llm = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "    \"\"\"\n",
    "    \n",
    "    text_splitter: TextSplitter\n",
    "    \"\"\"TextSplitter used to split up the input text if necessary (e.g. input text is too long to use for a single call\n",
    "    to LLM)\n",
    "    \"\"\"\n",
    "    question_prompt: BasePromptTemplate = refine_prompts.PROMPT\n",
    "    \"\"\"Question prompt for refine chain\n",
    "    \"\"\"\n",
    "    refine_prompt: BasePromptTemplate = refine_prompts.REFINE_PROMPT\n",
    "    \"\"\"Refine prompt for refine chain\n",
    "    \"\"\"\n",
    "\n",
    "    # def _call(\n",
    "    #     self,\n",
    "    #     inputs: Dict[str, Any],\n",
    "    #     run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    # ) -> Dict[str, str]:\n",
    "    #     text = inputs[self.prompt.input_variables[0]] # do some input validation here. checking that there is only one key in the dict\n",
    "    #     docs = self.text_splitter.create_documents([text])\n",
    "    #     with open(\"doc_nums.txt\", \"a\") as myfile:\n",
    "    #         myfile.write(f\"appended text {len(docs)}\")\n",
    "    #     refine_chain = load_summarize_chain(self.llm, chain_type=\"refine\")\n",
    "    #     output = refine_chain({\"input_documents\": docs}, return_only_outputs=True)\n",
    "    #     return {'text': output['output_text']}\n",
    "\n",
    "\n",
    "    def generate( # this is a hack and should be refactored\n",
    "        self,\n",
    "        input_list: List[Dict[str, Any]],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Generate LLM result from inputs.\"\"\"\n",
    "        # prompts, stop = self.prep_prompts(input_list, run_manager=run_manager)\n",
    "        # output = self.llm.generate_prompt(\n",
    "        #     prompts,\n",
    "        #     stop,\n",
    "        #     callbacks=run_manager.get_child() if run_manager else None,\n",
    "        #     **self.llm_kwargs,\n",
    "        # )\n",
    "        print('question prompot', self.question_prompt)\n",
    "        refine_chain = load_summarize_chain(\n",
    "            self.llm, chain_type=\"refine\",\n",
    "            refine_prompt=self.refine_prompt,\n",
    "            question_prompt=self.question_prompt, \n",
    "        )\n",
    "        generations = []\n",
    "        for input_item in input_list:\n",
    "            text = input_item[self.prompt.input_variables[0]] # do some input validation here. checking that there is only one key in the dict\n",
    "            docs = self.text_splitter.create_documents([text])\n",
    "            output = refine_chain(\n",
    "                {\"input_documents\": docs}, return_only_outputs=True\n",
    "            )\n",
    "            generations.append([Generation(text=output[\"output_text\"])])\n",
    "        return LLMResult(generations=generations)\n",
    "    \n",
    "    \"\"\"\n",
    "    todo: implement async call\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e029d1-80f5-4f8f-927e-28782bfda183",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = OpenAI()\n",
    "llm2 = OpenAI() # can probably just use one variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d730d806-1f4e-4be4-9f3b-a72a1bf891c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_text_splitter = CharacterTextSplitter(separator = \".\", chunk_size=1000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cf809dc-8d50-4ffa-b06c-e9cadcb3d6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"{text}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"], template=prompt_template\n",
    ") # kinda hacky but works\n",
    "refineLLMChain = RefineLLMChain(llm=llm1, prompt=prompt, text_splitter=refine_text_splitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef0ec8-f851-40b0-9c42-3078b380eaf8",
   "metadata": {},
   "source": [
    "Test the output of the custom RefineLLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a982ffa3-6248-47a7-baa4-6c816379baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question prompot input_variables=['text'] output_parser=None partial_variables={} template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:' template_format='f-string' validate_template=True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': '\\n\\nThree years ago, the US launched the \"great American comeback\" and the results are impressive. Jobs are booming, poverty is plummeting, and the US is highly respected again. The mentality of American decline and the downsizing of America\\'s destiny have been rejected, resulting in an increase in wealth, power, and prestige. Since taking office, President Trump has slashed a record number of job-killing regulations, enacted historic and record-setting tax cuts, and fought for fair and reciprocal trade agreements, resulting in the creation of 7 million new jobs and an unemployment rate lower than any other administration in the history of the US. We have totally rejected the downsizing and are moving forward at a pace that was unimaginable just a short time ago. The economy is the best it has ever been, the military is rebuilt with unmatched power, our borders are secure, families are flourishing, values are renewed, pride is restored, and the state of our Union is stronger than ever before. The vision laid out demonstrates how the US is building the worldâ€™s most prosperous and inclusive society. Unemployment for African American, Hispanic American, and Asian Americans has also reached the lowest levels in history, demonstrating the success of the reversal of failed economic policies.'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refineLLMChain({\"text\": docs[0].page_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1931de4d-c1f2-4e27-9643-f3b8dc393540",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(llm2, chain_type=\"refine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e103f124-afae-4f77-aa64-308ac14de58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=refineLLMChain,\n",
    "    combine_document_chain=refine_chain,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c6321-c142-4ae5-b94f-a0c57fa6babb",
   "metadata": {},
   "source": [
    "Test the custom MapReduceDocumentChain that uses a refine summarizing strategy for both the map and reduce steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb05046-7f8f-425a-967d-cace6de417a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question prompot input_variables=['text'] output_parser=None partial_variables={} template='Write a concise summary of the following:\\n\\n\\n\"{text}\"\\n\\n\\nCONCISE SUMMARY:' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "output = map_reduce_chain({\"input_documents\": docs[:3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a5881-bc51-4289-be5c-7fe6d8099700",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b89baf-e37e-4e88-91d6-ad2f118afe1f",
   "metadata": {},
   "source": [
    "### Now we will use a custom MapReduceDocumentsChain to summarize and contrast the State of the Union speeches of President Obama, Trump, and Biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e102c32-cba4-434c-a1b4-9d0f9563a345",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following:\n",
    "\n",
    "{text}\n",
    "\n",
    "CONCISE SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\n",
    "refine_template = (\n",
    "    \"Your job is to produce a final summary constrasting the state of the union speeches of President Biden, Trump, and Obama.\\n\"\n",
    "    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n",
    "    \"We have the opportunity to refine the existing summary\"\n",
    "    \"(only if needed) with some more context below.\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"{text}\\n\"\n",
    "    \"------------\\n\"\n",
    "    \"Given the new context, refine the original summary.\"\n",
    "    \"If the context isn't useful, return the original summary.\"\n",
    ")\n",
    "refine_prompt = PromptTemplate(\n",
    "    input_variables=[\"existing_answer\", \"text\"],\n",
    "    template=refine_template,\n",
    ")\n",
    "\n",
    "refine_chain = load_summarize_chain(llm2, chain_type=\"refine\", question_prompt=prompt_template, refine_prompt=refine_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b245326f-c7f6-49f9-9fe0-7a4d9ad0e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_text_splitter = CharacterTextSplitter(separator = \".\", chunk_size=3000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf053c2-eeb2-4c78-aa2e-7ff7b0a7e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"{text}\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"], template=prompt_template\n",
    ") # kinda hacky but works\n",
    "refineLLMChain = RefineLLMChain(llm=llm1, prompt=prompt, text_splitter=refine_text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ed997-4f17-4682-9ec5-353cbc4d367e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
    "\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=refineLLMChain,\n",
    "    combine_document_chain=refine_chain,\n",
    "    return_intermediate_steps=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e5adc-0bbe-4e24-b64b-1b7cc838b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = map_reduce_chain({\"input_documents\": docs})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
